import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import json, os

class ModelRegistry:
    """
    âœ… ëª¨ë¸ ìºì‹œ ê´€ë¦¬ í´ë˜ìŠ¤
    - ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ì „ì—­ì ìœ¼ë¡œ ì¬ì‚¬ìš©
    - ë„ë©”ì¸ë³„ Phase1/Phase2 ëª¨ë¸ ìºì‹œ
    """
    _cache = {}

    @classmethod
    def get(cls, phase1_model_name, phase2_model_name):
        key = f"{phase1_model_name}|{phase2_model_name}"

        if key not in cls._cache:
            print(f"ğŸ“¦ Loading models for: {key}")
            device = "cuda" if torch.cuda.is_available() else "cpu"

            # Phase1: Aspect Classifier
            aspect_tokenizer = AutoTokenizer.from_pretrained(phase1_model_name)
            aspect_model = AutoModelForSequenceClassification.from_pretrained(phase1_model_name).to(device).eval()

            # Phase2: Sentiment Classifier
            sent_tokenizer = AutoTokenizer.from_pretrained(phase2_model_name)
            sent_model = AutoModelForSequenceClassification.from_pretrained(phase2_model_name).to(device).eval()

            cls._cache[key] = {
                "device": device,
                "aspect_tokenizer": aspect_tokenizer,
                "aspect_model": aspect_model,
                "sent_tokenizer": sent_tokenizer,
                "sent_model": sent_model,
            }

        return cls._cache[key]
