import os
import re
import sys
from konlpy.tag import Okt
from collections import Counter
from wordcloud import WordCloud
from dotenv import load_dotenv

# ê²½ë¡œ ì„¤ì • (ë…ë¦½ ì‹¤í–‰ ì‹œ)
if __name__ == "__main__":
    current_dir = os.path.dirname(os.path.abspath(__file__))
    model_server_dir = os.path.dirname(current_dir)
    if model_server_dir not in sys.path:
        sys.path.insert(0, model_server_dir)

from utils.db_connect import get_connection

load_dotenv()

# ======================================
# ğŸ”¹ model_server ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œ)
# ======================================
def get_model_server_dir():
    """í˜„ì¬ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ model_server ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜"""
    current_file = os.path.abspath(__file__)  # generate_wordcloud_from_db.pyì˜ ì ˆëŒ€ ê²½ë¡œ
    utils_dir = os.path.dirname(current_file)  # utils ë””ë ‰í† ë¦¬
    model_server_dir = os.path.dirname(utils_dir)  # model_server ë””ë ‰í† ë¦¬
    return model_server_dir


# ======================================
# ğŸ”¹ ë¶ˆìš©ì–´ ë¡œë“œ í•¨ìˆ˜ (ì ˆëŒ€ê²½ë¡œ + ë¡œê·¸ í¬í•¨)
# ======================================
def load_stopwords(domain="steam", debug=False):
    stopwords = set()

    # ğŸ”¹ í˜„ì¬ íŒŒì¼(app/utils/generate_wordcloud_from_db.py) ê¸°ì¤€ ê²½ë¡œ
    current_dir = os.path.dirname(os.path.abspath(__file__))
    stopword_dir = os.path.join(current_dir, "stopwords")  # âœ… ê°™ì€ í´ë”
    base_path = os.path.join(stopword_dir, "base.txt")
    domain_path = os.path.join(stopword_dir, f"{domain}.txt")

    loaded_count = 0
    for path in [base_path, domain_path]:
        if os.path.exists(path):
            file_count = 0
            with open(path, "r", encoding="utf-8-sig") as f:
                for line in f:
                    # ê³µë°±, íƒ­, ê°œí–‰ ë¬¸ì ëª¨ë‘ ì œê±°í•˜ê³  ì •ì œ
                    word = line.strip().replace("\ufeff", "").replace("\t", "").replace(" ", "")
                    # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ˆê³  ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ê°€
                    if word and len(word) > 0:
                        stopwords.add(word)
                        file_count += 1
            loaded_count += file_count
            if debug:
                print(f"ğŸ“ ë¶ˆìš©ì–´ íŒŒì¼ ë¡œë“œ: {os.path.basename(path)} - {file_count}ê°œ ë‹¨ì–´")
        else:
            if debug:
                print(f"âš ï¸ ë¶ˆìš©ì–´ íŒŒì¼ ì—†ìŒ: {os.path.basename(path)}")
    
    if debug:
        print(f"âœ… ì´ ë¶ˆìš©ì–´ ë¡œë“œ: {len(stopwords)}ê°œ (domain={domain})")
    
    return stopwords


# ======================================
# ğŸ”¹ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜
# ======================================
def generate_wordcloud_from_db(product_id: int, domain="steam"):
    conn = get_connection()
    cursor = conn.cursor()

    # 0ï¸âƒ£ ì œí’ˆ ì •ë³´ ì¡°íšŒ (ì œí’ˆëª…, ë¸Œëœë“œë¥¼ ë¶ˆìš©ì–´ì— ì¶”ê°€í•˜ê¸° ìœ„í•´)
    cursor.execute(
        "SELECT product_name, brand FROM tb_product WHERE product_id = %s", (product_id,)
    )
    product_info = cursor.fetchone()
    product_name = product_info["product_name"] if product_info else None
    brand = product_info["brand"] if product_info else None

    # 1ï¸âƒ£ ë¦¬ë·° ë¶ˆëŸ¬ì˜¤ê¸°
    cursor.execute(
        "SELECT review_text FROM tb_review WHERE product_id = %s", (product_id,)
    )
    reviews = [r["review_text"] for r in cursor.fetchall() if r["review_text"]]

    if not reviews:
        conn.close()
        return None

    # 2ï¸âƒ£ í…ìŠ¤íŠ¸ ì •ì œ
    text_all = " ".join(reviews)
    text_all = re.sub(r"[^ã„±-ã…ê°€-í£a-zA-Z0-9\s]", " ", text_all)

    # 3ï¸âƒ£ í˜•íƒœì†Œ ë¶„ì„
    okt = Okt()
    tokens = [
        t for t, pos in okt.pos(text_all) if pos in ["Noun", "Adjective"] and len(t) > 1
    ]

    # 4ï¸âƒ£ ë¶ˆìš©ì–´ ì œê±°
    stopwords = load_stopwords(domain, debug=True)
    
    # ì œí’ˆëª…ê³¼ ë¸Œëœë“œë¥¼ ë¶ˆìš©ì–´ì— ì¶”ê°€
    if product_name:
        # ì œí’ˆëª…ì„ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ê° ë‹¨ì–´ë„ ì¶”ê°€
        product_words = product_name.split()
        for word in product_words:
            word_clean = word.strip()
            if word_clean and len(word_clean) > 1:
                stopwords.add(word_clean)
        # ì „ì²´ ì œí’ˆëª…ë„ ì¶”ê°€
        product_name_clean = product_name.strip()
        if product_name_clean:
            stopwords.add(product_name_clean)
        print(f"ğŸ“ ì œí’ˆëª… ë¶ˆìš©ì–´ ì¶”ê°€: {product_name} (ë‹¨ì–´: {product_words})")
    
    if brand:
        brand_clean = brand.strip()
        if brand_clean and len(brand_clean) > 1:
            stopwords.add(brand_clean)
            print(f"ğŸ“ ë¸Œëœë“œ ë¶ˆìš©ì–´ ì¶”ê°€: {brand}")
    
    # í† í° ì •ê·œí™” ë° ë¶ˆìš©ì–´ ì œê±°
    tokens_before = len(tokens)
    filtered_tokens = []
    removed_words = []
    
    for token in tokens:
        # í† í° ì •ê·œí™”: ê³µë°± ì œê±°
        normalized_token = token.strip()
        if not normalized_token:
            continue
            
        # ë¶ˆìš©ì–´ ì²´í¬: ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜, ì œí’ˆëª…/ë¸Œëœë“œì— í¬í•¨ë˜ëŠ” ê²½ìš° ì œê±°
        should_remove = False
        remove_reason = None
        
        # 1. ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë¶ˆìš©ì–´ ì²´í¬
        if normalized_token in stopwords:
            should_remove = True
            remove_reason = "ë¶ˆìš©ì–´ ì¼ì¹˜"
        # 2. ì œí’ˆëª…ì— í¬í•¨ë˜ëŠ” ê²½ìš° (ë¶€ë¶„ ë§¤ì¹­)
        elif product_name and normalized_token in product_name:
            should_remove = True
            remove_reason = "ì œí’ˆëª… í¬í•¨"
        # 3. ë¸Œëœë“œì— í¬í•¨ë˜ëŠ” ê²½ìš° (ë¶€ë¶„ ë§¤ì¹­)
        elif brand and normalized_token in brand:
            should_remove = True
            remove_reason = "ë¸Œëœë“œ í¬í•¨"
        
        if should_remove:
            removed_words.append(normalized_token)
        else:
            filtered_tokens.append(normalized_token)
    
    tokens = filtered_tokens
    tokens_after = len(tokens)
    
    # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
    print(f"ğŸ” ë¶ˆìš©ì–´ ì œê±° í†µê³„:")
    print(f"   - ì œê±° ì „ í† í° ìˆ˜: {tokens_before}")
    print(f"   - ì œê±° í›„ í† í° ìˆ˜: {tokens_after}")
    print(f"   - ì œê±°ëœ í† í° ìˆ˜: {tokens_before - tokens_after}")
    if removed_words:
        removed_counter = Counter(removed_words)
        print(f"   - ì œê±°ëœ ìƒìœ„ 10ê°œ ë‹¨ì–´: {dict(removed_counter.most_common(10))}")

    # 5ï¸âƒ£ ë¹ˆë„ ê³„ì‚°
    freq = dict(Counter(tokens).most_common(200))
    if not freq:
        conn.close()
        return None

    # 6ï¸âƒ£ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ë° ì €ì¥ (ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©)
    model_server_dir = get_model_server_dir()
    static_dir = os.path.join(model_server_dir, "static", "wordclouds")

    os.makedirs(static_dir, exist_ok=True)

    save_path = os.path.join(static_dir, f"product_{product_id}_wc.png")
    public_path = f"/static/wordclouds/product_{product_id}_wc.png"

    wc = WordCloud(
        font_path="malgun.ttf",
        width=1000,
        height=700,
        background_color="white",
        colormap="tab10",
    ).generate_from_frequencies(freq)

    wc.to_file(save_path)

    # 7ï¸âƒ£ DB ê²½ë¡œ ì—…ë°ì´íŠ¸
    cursor.execute(
        """
        UPDATE tb_productDashboard
        SET wordcloud_path = %s
        WHERE product_id = %s
    """,
        (public_path, product_id),
    )
    conn.commit()
    conn.close()
    return public_path


# ======================================
# ğŸ”¹ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸)
# ======================================
if __name__ == "__main__":
    from utils.db_connect import init_db_pool, close_db_pool
    
    # í…ŒìŠ¤íŠ¸ìš© product_id
    product_id = int(sys.argv[1]) if len(sys.argv) > 1 else 1008
    domain = sys.argv[2] if len(sys.argv) > 2 else "electronics"
    
    try:
        print("ğŸ”§ DB Connection Pool ì´ˆê¸°í™” ì¤‘...")
        init_db_pool()
        
        print(f"\nğŸŒˆ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹œì‘ (product_id={product_id}, domain={domain})")
        wc_path = generate_wordcloud_from_db(product_id, domain)
        
        if wc_path:
            print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ! ì›Œë“œí´ë¼ìš°ë“œ ê²½ë¡œ: {wc_path}")
        else:
            print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        print("\nğŸ§¹ DB Connection Pool ì •ë¦¬ ì¤‘...")
        close_db_pool()