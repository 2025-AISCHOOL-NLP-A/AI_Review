import os
import re
import sys
from konlpy.tag import Okt
from collections import Counter
from wordcloud import WordCloud
from dotenv import load_dotenv

# ê²½ë¡œ ì„¤ì • (ë…ë¦½ ì‹¤í–‰ ì‹œ)
if __name__ == "__main__":
    current_dir = os.path.dirname(os.path.abspath(__file__))
    model_server_dir = os.path.dirname(current_dir)
    if model_server_dir not in sys.path:
        sys.path.insert(0, model_server_dir)

from utils.db_connect import get_connection

load_dotenv()

# ======================================
# ğŸ”¹ model_server ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œ)
# ======================================
def get_model_server_dir():
    """í˜„ì¬ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ model_server ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜"""
    current_file = os.path.abspath(__file__)  # generate_wordcloud_from_db.pyì˜ ì ˆëŒ€ ê²½ë¡œ
    utils_dir = os.path.dirname(current_file)  # utils ë””ë ‰í† ë¦¬
    model_server_dir = os.path.dirname(utils_dir)  # model_server ë””ë ‰í† ë¦¬
    return model_server_dir


# ======================================
# ğŸ”¹ ë¶ˆìš©ì–´ ë¡œë“œ í•¨ìˆ˜ (ì ˆëŒ€ê²½ë¡œ + ë¡œê·¸ í¬í•¨)
# ======================================
def load_stopwords(domain="steam", debug=False):
    stopwords = set()

    # ğŸ”¹ í˜„ì¬ íŒŒì¼(app/utils/generate_wordcloud_from_db.py) ê¸°ì¤€ ê²½ë¡œ
    current_dir = os.path.dirname(os.path.abspath(__file__))
    stopword_dir = os.path.join(current_dir, "stopwords")  # âœ… ê°™ì€ í´ë”
    base_path = os.path.join(stopword_dir, "base.txt")
    domain_path = os.path.join(stopword_dir, f"{domain}.txt")

    loaded_count = 0
    for path in [base_path, domain_path]:
        if os.path.exists(path):
            file_count = 0
            with open(path, "r", encoding="utf-8-sig") as f:
                for line in f:
                    # ê³µë°±, íƒ­, ê°œí–‰ ë¬¸ì ëª¨ë‘ ì œê±°í•˜ê³  ì •ì œ
                    word = line.strip().replace("\ufeff", "").replace("\t", "").replace(" ", "")
                    # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ˆê³  ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ê°€
                    if word and len(word) > 0:
                        stopwords.add(word)
                        file_count += 1
            loaded_count += file_count
            if debug:
                print(f"ğŸ“ ë¶ˆìš©ì–´ íŒŒì¼ ë¡œë“œ: {os.path.basename(path)} - {file_count}ê°œ ë‹¨ì–´")
        else:
            if debug:
                print(f"âš ï¸ ë¶ˆìš©ì–´ íŒŒì¼ ì—†ìŒ: {os.path.basename(path)}")
    
    if debug:
        print(f"âœ… ì´ ë¶ˆìš©ì–´ ë¡œë“œ: {len(stopwords)}ê°œ (domain={domain})")
    
    return stopwords


# ======================================
# ğŸ”¹ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜
# ======================================
def generate_wordcloud_from_db(product_id: int, domain="steam", start_date: str = None, end_date: str = None):
    conn = get_connection()
    cursor = conn.cursor()

    # 0ï¸âƒ£ ì œí’ˆ ì •ë³´ ì¡°íšŒ (ì œí’ˆëª…, ë¸Œëœë“œë¥¼ ë¶ˆìš©ì–´ì— ì¶”ê°€í•˜ê¸° ìœ„í•´)
    cursor.execute(
        "SELECT product_name, brand FROM tb_product WHERE product_id = %s", (product_id,)
    )
    product_info = cursor.fetchone()
    product_name = product_info["product_name"] if product_info else None
    brand = product_info["brand"] if product_info else None

    # 1ï¸âƒ£ ë¦¬ë·° ë¶ˆëŸ¬ì˜¤ê¸° (ê¸°ê°„ í•„í„° ì ìš©)
    where_clause = "WHERE product_id = %s"
    params = [product_id]
    if start_date:
        where_clause += " AND DATE(review_date) >= %s"
        params.append(start_date)
    if end_date:
        where_clause += " AND DATE(review_date) <= %s"
        params.append(end_date)

    cursor.execute(
        f"SELECT review_text FROM tb_review {where_clause}",
        tuple(params),
    )
    reviews = [r["review_text"] for r in cursor.fetchall() if r["review_text"]]

    if not reviews:
        conn.close()
        return None

    print(f"ğŸ“Š ì´ ë¦¬ë·° ìˆ˜: {len(reviews)}ê°œ")
    
    # ì„±ëŠ¥ ìµœì í™”: ë¦¬ë·°ê°€ ë§ì„ ê²½ìš° ìƒ˜í”Œë§ (ìµœëŒ€ 2000ê°œë¡œ ì¤„ì„ - ë©”ëª¨ë¦¬ ë¶€ì¡± ë°©ì§€)
    MAX_REVIEWS = 2000
    if len(reviews) > MAX_REVIEWS:
        import random
        # ìµœê·¼ ë¦¬ë·° ìš°ì„  ìƒ˜í”Œë§
        reviews_sample = reviews[-MAX_REVIEWS:] if len(reviews) > MAX_REVIEWS * 2 else random.sample(reviews, MAX_REVIEWS)
        print(f"âš¡ ì„±ëŠ¥ ìµœì í™”: {len(reviews)}ê°œ ì¤‘ {len(reviews_sample)}ê°œ ìƒ˜í”Œë§í•˜ì—¬ ì²˜ë¦¬")
        reviews = reviews_sample

    # 2ï¸âƒ£ í…ìŠ¤íŠ¸ ì •ì œ ë° ë°°ì¹˜ ì²˜ë¦¬
    print("ğŸ“ í…ìŠ¤íŠ¸ ì •ì œ ë° í˜•íƒœì†Œ ë¶„ì„ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    okt = Okt()
    
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë¦¬ë·°ë¥¼ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    REVIEW_BATCH_SIZE = 100  # í•œ ë²ˆì— ì²˜ë¦¬í•  ë¦¬ë·° ìˆ˜
    MAX_TEXT_LENGTH = 30000  # í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´ (ë¬¸ì)
    
    all_tokens = []
    num_batches = (len(reviews) + REVIEW_BATCH_SIZE - 1) // REVIEW_BATCH_SIZE
    
    for i in range(0, len(reviews), REVIEW_BATCH_SIZE):
        batch_reviews = reviews[i:i + REVIEW_BATCH_SIZE]
        batch_text = " ".join(batch_reviews)
        batch_text = re.sub(r"[^ã„±-ã…ê°€-í£a-zA-Z0-9\s]", " ", batch_text)
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ë” ì‘ê²Œ ë‚˜ëˆ„ê¸°
        if len(batch_text) > MAX_TEXT_LENGTH:
            # í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
            text_parts = []
            current_part = ""
            for char in batch_text:
                current_part += char
                if len(current_part) >= MAX_TEXT_LENGTH:
                    text_parts.append(current_part)
                    current_part = ""
            if current_part:
                text_parts.append(current_part)
            
            for part_idx, text_part in enumerate(text_parts):
                try:
                    ## ì›í˜• ì²˜ë¦¬ x ë²„ì „
                    part_tokens = [
                        t for t, pos in okt.pos(text_part) 
                        if pos in ["Noun", "Adjective"] and len(t) > 1
                    ]
                    all_tokens.extend(part_tokens)
                    ## ì›í˜• ì²˜ë¦¬ ë²„ì „
                    # part_tokens = []
                    # morphs = okt.pos(text_part, stem=True)
                    # for t, pos in morphs:
                    #     if pos in ["Noun", "Adjective"] and len(t) > 1: # ["Noun", "Adjective", "Verb"]
                    #         part_tokens.append(t)
                    all_tokens.extend(part_tokens)
                except Exception as e:
                    print(f"âš ï¸ ë°°ì¹˜ {i//REVIEW_BATCH_SIZE + 1}ì˜ ë¶€ë¶„ {part_idx + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {e}")
                    continue
        else:
            try:
                ## ì›í˜• ì²˜ë¦¬ x ë²„ì „
                batch_tokens = [
                    t for t, pos in okt.pos(batch_text) 
                    if pos in ["Noun", "Adjective"] and len(t) > 1
                ]
                ## ì›í˜• ì²˜ë¦¬ ë²„ì „
                # batch_tokens = []
                # morphs = okt.pos(batch_text, stem=True)
                # for t, pos in morphs:
                #     if pos in ["Noun", "Adjective"] and len(t) > 1: # ["Noun", "Adjective", "Verb"]
                #         batch_tokens.append(t)
                all_tokens.extend(batch_tokens)
            except Exception as e:
                print(f"âš ï¸ ë°°ì¹˜ {i//REVIEW_BATCH_SIZE + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {e}")
                continue
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        batch_num = i // REVIEW_BATCH_SIZE + 1
        print(f"   ì§„í–‰: {batch_num}/{num_batches} ë°°ì¹˜ ì™„ë£Œ ({len(all_tokens)}ê°œ í† í° ìˆ˜ì§‘)")
    
    tokens = all_tokens
    print(f"âœ… í˜•íƒœì†Œ ë¶„ì„ ì™„ë£Œ: {len(tokens)}ê°œ í† í° ì¶”ì¶œ")

    # 4ï¸âƒ£ ë¶ˆìš©ì–´ ì œê±°
    stopwords = load_stopwords(domain, debug=True)
    
    # ì œí’ˆëª…ê³¼ ë¸Œëœë“œë¥¼ ë¶ˆìš©ì–´ì— ì¶”ê°€
    if product_name:
        # ì œí’ˆëª…ì„ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ê° ë‹¨ì–´ë„ ì¶”ê°€
        product_words = product_name.split()
        for word in product_words:
            word_clean = word.strip()
            if word_clean and len(word_clean) > 1:
                stopwords.add(word_clean)
        # ì „ì²´ ì œí’ˆëª…ë„ ì¶”ê°€
        product_name_clean = product_name.strip()
        if product_name_clean:
            stopwords.add(product_name_clean)
        print(f"ğŸ“ ì œí’ˆëª… ë¶ˆìš©ì–´ ì¶”ê°€: {product_name} (ë‹¨ì–´: {product_words})")
    
    if brand:
        brand_clean = brand.strip()
        if brand_clean and len(brand_clean) > 1:
            stopwords.add(brand_clean)
            print(f"ğŸ“ ë¸Œëœë“œ ë¶ˆìš©ì–´ ì¶”ê°€: {brand}")
    
    # í† í° ì •ê·œí™” ë° ë¶ˆìš©ì–´ ì œê±°
    tokens_before = len(tokens)
    filtered_tokens = []
    removed_words = []
    
    # ë¶ˆìš©ì–´ í™•ì¸ìš© ë””ë²„ê¹…
    print(f"ğŸ” ë¶ˆìš©ì–´ ì„¸íŠ¸ í¬ê¸°: {len(stopwords)}ê°œ")
    test_words = ["ìƒˆë¼", "ë³‘ì‹ ", "ì”¨ë°œ"]
    for test_word in test_words:
        if test_word in stopwords:
            print(f"   âœ… '{test_word}' ë¶ˆìš©ì–´ì— í¬í•¨ë¨")
        else:
            print(f"   âŒ '{test_word}' ë¶ˆìš©ì–´ì— ì—†ìŒ!")
    
    for token in tokens:
        # í† í° ì •ê·œí™”: ê³µë°± ì œê±°
        normalized_token = token.strip()
        if not normalized_token:
            continue
            
        # ë¶ˆìš©ì–´ ì²´í¬: ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜, ë¶ˆìš©ì–´ê°€ í† í°ì— í¬í•¨ë˜ê±°ë‚˜, í† í°ì´ ë¶ˆìš©ì–´ì— í¬í•¨ë˜ëŠ” ê²½ìš° ì œê±°
        should_remove = False
        remove_reason = None
        
        # 1. ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë¶ˆìš©ì–´ ì²´í¬
        if normalized_token in stopwords:
            should_remove = True
            remove_reason = "ë¶ˆìš©ì–´ ì¼ì¹˜"
        # 2. ë¶ˆìš©ì–´ê°€ í† í°ì— í¬í•¨ë˜ëŠ” ê²½ìš° (ì˜ˆ: "ê°œìƒˆë¼" -> "ìƒˆë¼" í¬í•¨)
        elif any(sw in normalized_token for sw in stopwords if len(sw) > 1):
            should_remove = True
            remove_reason = "ë¶ˆìš©ì–´ í¬í•¨"
        # 3. í† í°ì´ ë¶ˆìš©ì–´ì— í¬í•¨ë˜ëŠ” ê²½ìš° (ì˜ˆ: "ìƒˆ" -> "ìƒˆë¼"ì— í¬í•¨, í•˜ì§€ë§Œ ì´ê±´ ì œì™¸)
        # 4. ì œí’ˆëª…ì— í¬í•¨ë˜ëŠ” ê²½ìš° (ë¶€ë¶„ ë§¤ì¹­)
        elif product_name and normalized_token in product_name:
            should_remove = True
            remove_reason = "ì œí’ˆëª… í¬í•¨"
        # 5. ë¸Œëœë“œì— í¬í•¨ë˜ëŠ” ê²½ìš° (ë¶€ë¶„ ë§¤ì¹­)
        elif brand and normalized_token in brand:
            should_remove = True
            remove_reason = "ë¸Œëœë“œ í¬í•¨"
        
        if should_remove:
            removed_words.append(normalized_token)
        else:
            filtered_tokens.append(normalized_token)
    
    tokens = filtered_tokens
    tokens_after = len(tokens)
    
    # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
    print(f"ğŸ” ë¶ˆìš©ì–´ ì œê±° í†µê³„:")
    print(f"   - ì œê±° ì „ í† í° ìˆ˜: {tokens_before}")
    print(f"   - ì œê±° í›„ í† í° ìˆ˜: {tokens_after}")
    print(f"   - ì œê±°ëœ í† í° ìˆ˜: {tokens_before - tokens_after}")
    if removed_words:
        removed_counter = Counter(removed_words)
        print(f"   - ì œê±°ëœ ìƒìœ„ 10ê°œ ë‹¨ì–´: {dict(removed_counter.most_common(10))}")

    # 5ï¸âƒ£ ë¹ˆë„ ê³„ì‚°
    freq = dict(Counter(tokens).most_common(200))
    if not freq:
        conn.close()
        return None

    # 6ï¸âƒ£ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ë° ì €ì¥ (ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©)
    model_server_dir = get_model_server_dir()
    static_dir = os.path.join(model_server_dir, "static", "wordclouds")

    os.makedirs(static_dir, exist_ok=True)

    suffix = ""
    if start_date or end_date:
        start_token = start_date.replace("-", "") if start_date else "start"
        end_token = end_date.replace("-", "") if end_date else "end"
        suffix = f"_{start_token}_{end_token}"

    save_path = os.path.join(static_dir, f"product_{product_id}_wc{suffix}.png")
    public_path = f"/static/wordclouds/product_{product_id}_wc{suffix}.png"

    wc = WordCloud(
        font_path="malgun.ttf",
        width=1000,
        height=700,
        background_color="white",
        colormap="tab10",
    ).generate_from_frequencies(freq)

    wc.to_file(save_path)

    # 7ï¸âƒ£ DB ê²½ë¡œ ì—…ë°ì´íŠ¸
    # ëŒ€ì‹œë³´ë“œ í…Œì´ë¸” ì—…ë°ì´íŠ¸ëŠ” ì „ì²´ ê¸°ê°„ ê¸°ë³¸ ê²½ë¡œì¼ ë•Œë§Œ ìˆ˜í–‰ (ê¸°ê°„ í•„í„° ì‹œì—ëŠ” íŒŒì¼ë§Œ ìƒì„±)
    if not start_date and not end_date:
        cursor.execute(
            """
            UPDATE tb_productDashboard
            SET wordcloud_path = %s
            WHERE product_id = %s
        """,
            (public_path, product_id),
        )
        conn.commit()
    conn.close()
    return public_path


# ======================================
# ğŸ”¹ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸)
# ======================================
if __name__ == "__main__":
    from utils.db_connect import init_db_pool, close_db_pool
    
    # í…ŒìŠ¤íŠ¸ìš© product_id
    product_id = int(sys.argv[1]) if len(sys.argv) > 1 else 1038
    domain = sys.argv[2] if len(sys.argv) > 2 else "electronics"
    
    try:
        print("ğŸ”§ DB Connection Pool ì´ˆê¸°í™” ì¤‘...")
        init_db_pool()
        
        print(f"\nğŸŒˆ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹œì‘ (product_id={product_id}, domain={domain})")
        wc_path = generate_wordcloud_from_db(product_id, domain)
        
        if wc_path:
            print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ! ì›Œë“œí´ë¼ìš°ë“œ ê²½ë¡œ: {wc_path}")
        else:
            print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        print("\nğŸ§¹ DB Connection Pool ì •ë¦¬ ì¤‘...")
        close_db_pool()
