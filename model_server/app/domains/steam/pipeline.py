import re
import torch
import numpy as np
import time
from typing import List, Dict, Any
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from app.models import ModelRegistry
from .keywords import BOOST_KEYWORDS, NEG_TRIGGERS
import json, os

CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.json")
cfg = json.load(open(CONFIG_PATH, encoding="utf-8"))
ASPECTS = cfg["aspect_labels"]

# =========================================================
# 1ï¸âƒ£ ë¬¸ì¥ ë¶„ë¦¬ (ì ‘ì†ì‚¬ í¬í•¨) - ìµœì í™” ë²„ì „
# =========================================================
def split_sentences(text, max_sentences=10, min_length=5):
    """
    ë¬¸ì¥ ë¶„ë¦¬ (ì„±ëŠ¥ ìµœì í™”)
    - ìµœëŒ€ ë¬¸ì¥ ìˆ˜ ì œí•œ (ë„ˆë¬´ ë§ì€ ë¬¸ì¥ ìƒì„± ë°©ì§€)
    - ìµœì†Œ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œê±°)
    """
    text = re.sub(r"([^.?!]*?)(?:ì§€ë§Œ|ê·¸ëŸ¬ë‚˜|í•˜ì§€ë§Œ|ë°˜ë©´ì—)\s*", r"\1. ", text)
    sentences = re.split(r'(?<=[.!?])\s+|[ã€‚ï¼ï¼Ÿ]|(?<=ë‹¤)\s', text)
    # í•„í„°ë§: ìµœì†Œ ê¸¸ì´ ì´ìƒì´ê³ , ìµœëŒ€ ë¬¸ì¥ ìˆ˜ ì œí•œ
    filtered = [s.strip() for s in sentences if s.strip() and len(s.strip()) >= min_length]
    # ìµœëŒ€ ë¬¸ì¥ ìˆ˜ ì œí•œ (ì•ë¶€ë¶„ ìš°ì„ )
    return filtered[:max_sentences]

# =========================================================
# 2ï¸âƒ£ í‚¤ì›Œë“œ ê¸°ë°˜ Boost
# =========================================================
def boost_aspects(text, probs_dict):
    for aspect, kws in BOOST_KEYWORDS.items():
        if any(k in text for k in kws):
            probs_dict[aspect] = min(probs_dict.get(aspect, 0) + cfg["boost_value"], 1.0)
    return probs_dict

# =========================================================
# 3ï¸âƒ£ Phase-1 ì¸¡ë©´ íƒì§€
# =========================================================
def detect_aspects_multi(text, threshold=0.35):
    reg = ModelRegistry.get(
        cfg["phase1_model"],
        cfg["phase2_model"]
    )
    tok, model = reg["aspect_tokenizer"], reg["aspect_model"]   # âœ… dict ì ‘ê·¼
    device = reg["device"]

    sentences = split_sentences(text)
    total = {}

    print(f"\nğŸ§  [DEBUG] ë¬¸ì¥ë³„ ì¸¡ë©´ í™•ë¥  ë¡œê·¸")
    for s in sentences:
        inputs = tok(s, return_tensors="pt", truncation=True, padding=True).to(device)
        with torch.no_grad():
            logits = model(**inputs).logits
            probs = torch.sigmoid(logits).cpu().numpy()[0]
        detected = {ASPECTS[i]: float(probs[i]) for i in range(len(ASPECTS))}
        detected = boost_aspects(s, detected)
        for asp, prob in detected.items():
            print(f" - {asp:6s} | {s[:35]:35s} â†’ {prob:.3f}")
            if prob >= threshold:
                total[asp] = max(total.get(asp, 0), prob)
    return total

# =========================================================
# 4ï¸âƒ£ ë¶€ì • í‚¤ì›Œë“œ ë³´ì •
# =========================================================
def polarity_correction(text, aspect, label):
    if any(k in text for k in NEG_TRIGGERS) and aspect in ["ìµœì í™”", "ë°¸ëŸ°ìŠ¤"]:
        return "ë¶€ì •"
    return label

# =========================================================
# 5ï¸âƒ£ Phase-2 ê°ì • ë¶„ë¥˜
# =========================================================
def analyze_sentiment(aspect, text):
    reg = ModelRegistry.get(cfg["phase1_model"], cfg["phase2_model"])
    tok, model = reg["sent_tokenizer"], reg["sent_model"]
    device = reg["device"]        # âœ… dictì´ë¯€ë¡œ [] ì•„ë‹˜, ì•„ë˜ì„œ ìˆ˜ì • ë°˜ì˜
    ctx = text if aspect not in text else text[text.find(aspect)-20:text.find(aspect)+30]
    inputs = tok(f"[{aspect}] {ctx}", return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        probs = torch.softmax(model(**inputs).logits, dim=-1).cpu().numpy()[0]
    pos, neg = float(probs[1]), float(probs[0])
    if abs(pos-neg) < cfg["margin"]:
        label = "ì¤‘ë¦½"
    else:
        label = "ê¸ì •" if pos>neg else "ë¶€ì •"
    return {
        "aspect": aspect,
        "POS": pos,
        "NEG": neg,
        "label": polarity_correction(text, aspect, label)
    }

# =========================================================
# 6ï¸âƒ£ í†µí•© ë¦¬ë·° ë¶„ì„
# =========================================================
def analyze_review(text):
    detected = detect_aspects_multi(text)
    results = [analyze_sentiment(asp, text) for asp in detected.keys()]
    return {"text": text, "aspects": list(detected.keys()), "results": results}

# =========================================================
# 7ï¸âƒ£ ë°°ì¹˜(Batch) ë¶„ì„ (ì—¬ëŸ¬ ë¦¬ë·° í•œ ë²ˆì—)
# =========================================================
def analyze_reviews(texts: List[str], debug: bool = False, batch_size: int = 16, threshold: float = 0.35) -> List[Dict[str, Any]]:
    """
    ì—¬ëŸ¬ ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ Batchë¡œ ë¶„ì„í•˜ì—¬ Steam í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    
    Args:
        texts: ë¶„ì„í•  ë¦¬ë·° í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        debug: ë””ë²„ê·¸ ëª¨ë“œ (ìƒì„¸ ë¡œê·¸ ì¶œë ¥)
        batch_size: ë°°ì¹˜ í¬ê¸° (Phase-1, Phase-2 ëª¨ë‘ì— ì ìš©)
        threshold: ì¸¡ë©´ íƒì§€ ì„ê³„ê°’
    
    Returns:
        Steam í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    if not texts:
        return []
    
    reg = ModelRegistry.get(cfg["phase1_model"], cfg["phase2_model"])
    aspect_tok, aspect_model = reg["aspect_tokenizer"], reg["aspect_model"]
    sent_tok, sent_model = reg["sent_tokenizer"], reg["sent_model"]
    device = reg["device"]
    
    start_total = time.monotonic()
    
    # =========================================================
    # Phase-1: ì¸¡ë©´ íƒì§€ (ë°°ì¹˜ ì²˜ë¦¬)
    # =========================================================
    print(f"[Steam] Phase-1 ì¸¡ë©´ íƒì§€ ì‹œì‘: {len(texts)}ê°œ ë¦¬ë·°")
    start_phase1 = time.monotonic()
    
    # ëª¨ë“  ë¦¬ë·°ì˜ ë¬¸ì¥ë“¤ì„ ìˆ˜ì§‘ (ìµœì í™”: ë¬¸ì¥ ìˆ˜ ì œí•œ)
    all_sentences: List[tuple[int, str]] = []  # (text_index, sentence)
    MAX_SENTENCES_PER_REVIEW = 8  # ë¦¬ë·°ë‹¹ ìµœëŒ€ ë¬¸ì¥ ìˆ˜ (ì„±ëŠ¥ ìµœì í™”)
    for ti, text in enumerate(texts):
        sentences = split_sentences(text, max_sentences=MAX_SENTENCES_PER_REVIEW, min_length=5)
        for sentence in sentences:
            all_sentences.append((ti, sentence))
    
    # Phase-1 ë°°ì¹˜ ì¶”ë¡ 
    detected_aspects_per_text: List[Dict[str, float]] = [{} for _ in texts]
    total_batches_phase1 = (len(all_sentences) + batch_size - 1) // batch_size
    
    print(f"[Steam] Phase-1: ì´ {len(all_sentences)}ê°œ ë¬¸ì¥, {total_batches_phase1}ê°œ ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì •")
    
    for i in range(0, len(all_sentences), batch_size):
        batch_num = i // batch_size + 1
        chunk = all_sentences[i:i + batch_size]
        sentences_batch = [item[1] for item in chunk]
        
        print(f"[Steam] Phase-1 ì§„í–‰: {batch_num}/{total_batches_phase1} ë°°ì¹˜ ì²˜ë¦¬ ì¤‘... ({len(chunk)}ê°œ ë¬¸ì¥)")
        
        # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
        inputs = aspect_tok(
            sentences_batch,
            return_tensors="pt",
            truncation=True,
            padding=True
        ).to(device)
        
        # ë°°ì¹˜ ì¶”ë¡ 
        with torch.no_grad():
            logits = aspect_model(**inputs).logits
            probs = torch.sigmoid(logits).cpu().numpy()
        
        # ê²°ê³¼ ì²˜ë¦¬
        detected_count = 0
        for (ti, sentence), prob_row in zip(chunk, probs):
            detected = {ASPECTS[j]: float(prob_row[j]) for j in range(len(ASPECTS))}
            detected = boost_aspects(sentence, detected)
            
            for asp, prob in detected.items():
                if prob >= threshold:
                    if asp not in detected_aspects_per_text[ti]:
                        detected_aspects_per_text[ti][asp] = prob
                        detected_count += 1
                    else:
                        detected_aspects_per_text[ti][asp] = max(detected_aspects_per_text[ti][asp], prob)
        
        print(f"[Steam] Phase-1 ë°°ì¹˜ {batch_num}/{total_batches_phase1} ì™„ë£Œ: {detected_count}ê°œ ì¸¡ë©´ íƒì§€")
    
    elapsed_phase1 = (time.monotonic() - start_phase1) * 1000
    print(f"[Steam] Phase-1 ì™„ë£Œ: {elapsed_phase1:.1f}ms")
    
    # =========================================================
    # Phase-2: ê°ì • ë¶„ë¥˜ (ë°°ì¹˜ ì²˜ë¦¬)
    # =========================================================
    print(f"[Steam] Phase-2 ê°ì • ë¶„ë¥˜ ì‹œì‘")
    start_phase2 = time.monotonic()
    
    # íƒì§€ëœ ì¸¡ë©´ë“¤ì„ ìˆ˜ì§‘: (text_index, aspect, text, context)
    sentiment_inputs: List[tuple[int, str, str, str]] = []
    for ti, text in enumerate(texts):
        for aspect in detected_aspects_per_text[ti].keys():
            ctx = text if aspect not in text else text[text.find(aspect)-20:text.find(aspect)+30]
            sentiment_inputs.append((ti, aspect, text, ctx))
    
    # Phase-2 ë°°ì¹˜ ì¶”ë¡ 
    sentiment_results: List[Dict[str, Any]] = [None] * len(sentiment_inputs)
    total_batches_phase2 = (len(sentiment_inputs) + batch_size - 1) // batch_size
    
    print(f"[Steam] Phase-2: ì´ {len(sentiment_inputs)}ê°œ ì¸¡ë©´-ê°ì • ìŒ, {total_batches_phase2}ê°œ ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì •")
    
    for i in range(0, len(sentiment_inputs), batch_size):
        batch_num = i // batch_size + 1
        chunk = sentiment_inputs[i:i + batch_size]
        
        print(f"[Steam] Phase-2 ì§„í–‰: {batch_num}/{total_batches_phase2} ë°°ì¹˜ ì²˜ë¦¬ ì¤‘... ({len(chunk)}ê°œ ì¸¡ë©´-ê°ì • ìŒ)")
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ ìƒì„±: "[aspect] context"
        inputs_texts = [f"[{item[1]}] {item[3]}" for item in chunk]
        
        # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
        inputs = sent_tok(
            inputs_texts,
            return_tensors="pt",
            truncation=True,
            padding=True
        ).to(device)
        
        # ë°°ì¹˜ ì¶”ë¡ 
        with torch.no_grad():
            logits = sent_model(**inputs).logits
            probs = torch.softmax(logits, dim=-1).cpu().numpy()
        
        # ê²°ê³¼ ì²˜ë¦¬
        pos_count = 0
        neg_count = 0
        neu_count = 0
        for idx, (ti, aspect, text, _), prob_row in zip(range(i, i + len(chunk)), chunk, probs):
            pos, neg = float(prob_row[1]), float(prob_row[0])
            if abs(pos - neg) < cfg["margin"]:
                label = "ì¤‘ë¦½"
                neu_count += 1
            else:
                label = "ê¸ì •" if pos > neg else "ë¶€ì •"
                if label == "ê¸ì •":
                    pos_count += 1
                else:
                    neg_count += 1
            
            label = polarity_correction(text, aspect, label)
            sentiment_results[idx] = {
                "aspect": aspect,
                "POS": pos,
                "NEG": neg,
                "label": label
            }
        
        print(f"[Steam] Phase-2 ë°°ì¹˜ {batch_num}/{total_batches_phase2} ì™„ë£Œ: ê¸ì • {pos_count}, ë¶€ì • {neg_count}, ì¤‘ë¦½ {neu_count}")
    
    elapsed_phase2 = (time.monotonic() - start_phase2) * 1000
    print(f"[Steam] Phase-2 ì™„ë£Œ: {elapsed_phase2:.1f}ms")
    
    # =========================================================
    # ê²°ê³¼ ì¬êµ¬ì„±
    # =========================================================
    # sentiment_inputsì™€ sentiment_resultsë¥¼ (text_index, aspect)ë¥¼ í‚¤ë¡œ ë§¤í•‘
    result_map: Dict[tuple[int, str], Dict[str, Any]] = {}
    for (ti, aspect, _, _), result in zip(sentiment_inputs, sentiment_results):
        if result:
            result_map[(ti, aspect)] = result
    
    # ê° ë¦¬ë·°ë³„ë¡œ ê²°ê³¼ ì¬êµ¬ì„±
    outputs: List[Dict[str, Any]] = []
    for ti, text in enumerate(texts):
        results = []
        detected_aspects = list(detected_aspects_per_text[ti].keys())
        
        # íƒì§€ëœ ì¸¡ë©´ë³„ë¡œ ê²°ê³¼ ë§¤í•‘
        for aspect in detected_aspects:
            if (ti, aspect) in result_map:
                results.append(result_map[(ti, aspect)])
        
        outputs.append({
            "text": text,
            "aspects": detected_aspects,
            "results": results
        })
    
    elapsed_total = (time.monotonic() - start_total) * 1000
    print(f"[Steam] ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: reviews={len(texts)}, {elapsed_total:.1f}ms")
    
    if debug:
        print(f"[Steam] ë°°ì¹˜ ê²°ê³¼ ê°œìˆ˜: {len(outputs)}")
    
    return outputs
